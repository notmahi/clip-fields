<!DOCTYPE html>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
</script>
<html>

<head>
  <meta charset="utf-8">
  <link rel="icon" href="https://mahis.life/favicon.ico" type="image/x-icon" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta property="og:title" content="CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory">
  <meta property="og:description"
    content="Teaching robots in the real world to respond to natural language queries with zero human labels — using pretrained large language models (LLMs), visual language models (VLMs), and neural fields.">
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory">
  <meta property="og:image" content="https://notmahi.github.io/clip-fields/mfiles/data_processing.jpeg" />
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory">
  <meta name="twitter:description"
    content="Teaching robots in the real world to respond to natural language queries with zero human labels — using pretrained large language models (LLMs), visual language models (VLMs), and neural fields.">
  <meta name="twitter:image" content="https://notmahi.github.io/clip-fields/mfiles/data_processing.jpeg" />
  <meta name="twitter:creator" content="@notmahi" />
  <link rel="shortcut icon" href="img/favicon.png">
  <link rel="stylesheet" href="css/simple-grid.css">
  <title>CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory</title>
  <script>
    function retime(duration, class_name) {
      var videos = document.getElementsByClassName(class_name);
      for (var i = 0; i < videos.length; i++) {
        var video = videos[i];
        video.onloadeddata = function () {
          this.playbackRate = this.duration / duration;
        };
      }
    }

    function monitor(replay_name) {
      var div = document.getElementById(replay_name);
      div.style.opacity = 1.0;
    }

    function replay(class_name, replay_name) {
      var video = document.getElementById(class_name);
      video.currentTime = 0;
      video.play();
      var div = document.getElementById(replay_name);
      div.style.opacity = 0.0;
    }

    function seek(class_id, progress_id, time, endtime) {
      var video = document.getElementById(class_id);
      var progress = document.getElementById(progress_id);
      video.currentTime = time;
      video.supposedEndTime = endtime;
      video.supposedStartTime = time;
      progress.style.transitionDuration = "0s";
      progress.value = Math.round((video.currentTime / video.duration) * 100);
      progress.style.transitionDuration = "0.5s";
      video.addEventListener("timeupdate", function (event) {
        target = event.target;
        if (target.currentTime >= target.supposedEndTime) {
          target.currentTime = target.supposedStartTime; // change time index here
        }
      }, false);
    }

    function progressLoop(video_class_id, progress_id) {
      var video = document.getElementById(video_class_id);
      var progress = document.getElementById(progress_id);
      console.log(video.currentTime / video.duration);
      function innerLoop() {
        if (video !== null && (video.currentTime / video.duration)) {
          progress.value = Math.round((video.currentTime / video.duration) * 100);
        }
        window.requestAnimationFrame(innerLoop);
      }
      innerLoop();
    }
  </script>
  <style>
    .replay {
      font-size: 1.5em;
      color: #00A2FF;
      text-decoration: none;
    }
  </style>
</head>

<body>
  <div class="jumbotron">
    <div class="container">
      <div class="row">
        <div class="col-12 center">
          <h1>CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory</h1>
        </div>
        <div class="col-1 hidden-sm"></div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="https://arxiv.org/abs/2210.05663">
            <h3 style="color: #dd00ff">Paper</h3>
          </a>
        </div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="https://github.com/notmahi/clip-fields">
            <h3 style="color: #dd00ff">Code</h3>
          </a>
        </div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="https://osf.io/famgv/">
            <h3 style="color: #dd00ff">Data</h3>
          </a>
        </div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="more/bibtex.txt">
            <h3 style="color: #dd00ff">Bibtex</h3>
          </a>
        </div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="https://youtu.be/bKu7GvRiSQU">
            <h3 style="color: #dd00ff">Video</h3>
          </a>
        </div>
      </div>
      <div class="row">
        <div class="col-4 center">
          <p><a href="https://mahis.life">Mahi Shafiullah<sup>1<a href="#footnote">†</a></sup></a></p>
        </div>
        <div class="col-4 center">
          <p><a href="https://cpaxton.github.io/">Chris Paxton<sup>2</sup></a></p>
        </div>
        <div class="col-4 center">
          <p><a href="https://lerrelpinto.com">Lerrel Pinto<sup>1</sup></a></p>
        </div>
        <div class="col-2 center">
          <p></p>
        </div>
        <div class="col-4 center">
          <p><a href="https://soumith.ch/">Soumith Chintala<sup>2</sup></a></p>
        </div>
        <div class="col-4 center">
          <p><a href="#">Arthur Szlam<sup>2</sup></a></p>
        </div>

      </div>
      <div class="row">
        <div class="col-6 center">
          <h3>1: New York University</h3>
        </div>
        <div class="col-6 center">
          <h3>2: Meta AI</h3>
        </div>
      </div>

      <div class="row">
        <div class="col-12">
          <p>
            <strong>Tl;dr</strong> CLIP-Field is a novel weakly supervised approach for learning a semantic robot
            memory that can respond to natural language queries solely from raw RGB-D and odometry data with no extra
            human labelling.
            It combines the image and language understanding capabilites of novel vision-language models (VLMs) like
            CLIP, large language models like sentence BERT, and open-label object detection models like Detic, and
            with spatial understanding capabilites of neural radiance field (NeRF) style architectures to
            build a spatial database that holds semantic information in it.
          </p>
        </div>
      </div>

      <!--Abstract-->
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom" id="abstract_tag">Abstract <span id="hide_logo">↓</span></h2>
          <p id="abstract_text">
            We propose CLIP-Fields, an implicit scene model that can be trained with no direct human supervision.
            This model learns a mapping from spatial locations to semantic embedding vectors.
            The mapping can then be used for a variety of tasks, such as segmentation, instance identification, semantic
            search over space, and view localization.
            Most importantly, the mapping can be trained with supervision coming only from web-image and web-text
            trained models such as CLIP, Detic, and Sentence-BERT.
            When compared to baselines like Mask-RCNN, our method outperforms on few-shot instance identification or
            semantic segmentation on the HM3D dataset with only a fraction of the examples.
            Finally, we show that using CLIP-Fields as a scene memory, robots can perform semantic navigation in
            real-world environments.
          </p>
        </div>
      </div>
    </div>

    <!--Videos-->
    <div class="container">
      <div class="row">
        <div class="col-12" style="width: 100%">
          <h2 class="center m-bottom" id="experiment_tag">Real World Robot Experiments</h2>
          <p>
            In these experiments, the robot is navigating the real world environements to "go and look at" the objects
            that are described by the query, which we expect to make accomplishing many downstream tasks possible,
            simply from natural language queries.
          </p>
          <div class="col-12 img" id="nyu_hero_video">
            <h4>Robot queries in a real lab kitchen setup.</h4>
            <video id="nyu_robot_run" class="center" style="width: 100%" muted autoplay loop>
              <source src="./mfiles/nyu_robot_run_clipped_small.mp4" type="video/mp4">
            </video>
            <progress id="progress_nyu" max="100" value="0">Progress</progress>
            <div style="margin: none; width: 100%;">
              <button onclick="seek('nyu_robot_run', 'progress_nyu',  0, 17.33)" class="seek_button" id="nyu_wash"
                style="width: calc((1733% - 0%) / 55.75);">Wash my dishes</button>
              <button onclick="seek('nyu_robot_run', 'progress_nyu',  17.34, 29.00)" class="seek_button" id="nyu_trash"
                style="width: calc((2900% - 1734%) / 55.75);">Throw out my
                trash</button>
              <button onclick=" seek('nyu_robot_run', 'progress_nyu', 29.05, 43.2)" class="seek_button" id="nyu_coffee"
                style="width: calc((4320% - 2905%) / 55.75);">Make me a
                coffee</button>
              <button onclick="seek('nyu_robot_run', 'progress_nyu',  43.21, 55.03)" class="seek_button" id="nyu_lunch"
                style="width: calc((5503% - 4321%) / 55.75);">Warm up my
                lunch</button>
              <button onclick="seek('nyu_robot_run', 'progress_nyu',  0, 55.03)" class="seek_button" id="nyu_full"
                style="width: calc((5550% - 0%) / 55.75);">Full video</button>
            </div>
          </div>
          <div class="col-12 img" id="pit_hero_video">
            <h4>Robot queries in a real lounge/library setup.</h4>
            <video id="pit_robot_run" class="center" style="width: 100%" muted autoplay loop>
              <source src="./mfiles/pit_robot_run_clipped_small.mp4" type="video/mp4">
            </video>

            <progress id="progress_pit" max="100" value="0">Progress</progress>
            <div style="margin: none; width: 100%;">
              <button onclick="seek('pit_robot_run', 'progress_pit',  0, 9.66)" class="seek_button" id="pit_bookshelf"
                style="width: calc(966% / 58.5);">Bookshelf</button>
              <button onclick="seek('pit_robot_run', 'progress_pit',  9.67, 31.5)" class="seek_button" id="pit_relax"
                style="width: calc((3150% - 967%) / 58.5);">Sit down and
                relax</button>
              <button onclick="seek('pit_robot_run', 'progress_pit',  31.52, 43.33)" class="seek_button" id="pit_write"
                style="width: calc((4333% - 3152%) / 58.5);">Write a novel</button>
              <button onclick="seek('pit_robot_run', 'progress_pit',  43.34, 57.9)" class="seek_button" id="pit_putdown"
                style="width: calc((5790% - 4335%) / 58.5);">Put down my
                novel</button>
              <button onclick="seek('pit_robot_run', 'progress_pit',  0, 57.9)" class="seek_button" id="pit_full"
                style="width: 100%;">Full
                video</button>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="container">
      <div class="row">
        <div class="col-12" style="width: 100%">
          <h2 class="center m-bottom">Interactive Demonstration</h2>
          <p>
            In this interactive demo, we show a heatmap of association between environment points and natural language
            queries made by a trained CLIP-field. Note that this model was trained without any human labels, and none of
            these phrases ever appeared in the training set.
          </p>
          <iframe src="kitchen.html" title="NYU Kitchen interactive demo" width="100%" height="720px"></iframe>
        </div>
      </div>
    </div>

    <!--Image-->
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Method</h2>
          <p>CLIP-Fields is based off of a series of simple ideas:
          <ul style="font-size: 1.125rem;font-weight: 200;line-height: 1.8">
            <li>Webscale models like CLIP and Detic provide lots of semantic information about objects that can be used
              for robot tasks, but don't encode spatial qualities of this information.
            </li>
            <li>NeRF-like approaches, on the other hand, have recently shown that they can capture very detailed scene
              information.
            </li>
            <li>We can combine these two, using a novel contrastive loss in order to capture scene-specific embeddings.
              We supervise multiple "heads," including object detection and CLIP, based on these webscale vision models,
              which allows us to do open-vocabulary queries at test time.
            </li>
          </ul>
          </p>
        </div>
      </div>
    </div>

    <div class="container">
      <div class="row">
        <div class="col-12 center img">
          <picture>
            <source type="image/avif" srcset="./mfiles/data_processing.avif" style="width:100%">
            <img class="center" src="./mfiles/data_processing.jpeg" style="width:100%"></img>
          </picture>
          <p>
            We collect our real world data using an <strong>iPhone 13 Pro</strong>, whose LiDAR scanner gives us RGB-D
            and odometry
            information, which we use to establish a pixel-to-real world coordinate correspondense.
          </p>
          <p>
            We use pre-trained models such as <a href="https://github.com/facebookresearch/Detic"
              target="_blank">Detic</a> and <a href="https://github.com/isl-org/lang-seg" target="_blank">LSeg</a> to
            extract the open-label semantic annotations from the RGB images, and use the labels to get <a
              href="https://huggingface.co/sentence-transformers">Sentence-BERT</a> encoding, and proposed bounding
            boxes to get <a href="https://github.com/openai/CLIP">CLIP</a> visual encoding.
            Note that we need to use <i><strong>no human labelling at all</strong></i> for training our models, and all
            of our supervision comes from pre-trained web-scale language models or VLMs.
          </p>
        </div>
      </div>
    </div>

    <div class="container">
      <div class="row">
        <div class="col-12 center img">
          <picture>
            <source type="image/avif" srcset="./mfiles/arch.avif" style="width:100%">
            <img class="center" src="./mfiles/arch.jpeg" style="width:100%"></img>
          </picture>
          <p>
            Our model is an implicit function that maps each 3-dimensional spatial location to a higher dimensional
            representation vector. Each of the vectors contain both the language-based and vision-based semantic
            embeddings of the content of location (x, y, z).
          </p>
          <p>
            The trunk of our model is an <a href="https://nvlabs.github.io/instant-ngp/" target="_blank">instant neural
              graphics primitive</a> based hash-grid architecture as our scene representation, and use MLPs to map them
            to higher dimensions that match the output dimension for embedding models such as Sentence-BERT or CLIP.
          </p>
        </div>
      </div>
    </div>

    <div class="container">
      <div class="row">
        <div class="col-12 center img">
          <video style="width: 100%" id="desc_1" onended="monitor('replay_1')" playsinline muted autoplay>
            <source src="./mfiles/training.mp4" type="video/mp4">
          </video>
          <span class="replay" id="replay_1" onclick="replay('desc_1', 'replay_1')">replay</span>
          <p>
          <p>
            We train with a contrastive loss that pushes the model's learned embeddings to be close to similar
            embeddings in the labeled datasets and far away from dissimilar embeddings. The contrastive training also
            helps us denoise the (sometimes) noisy labels given by the training models.
          </p>
        </div>
      </div>
    </div>

    <!--Experiments-->
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Experiments</h2>
          <picture>
            <source type="image/avif" srcset="./mfiles/query_navigation.avif" style="width:100%">
            <img class="center" src="./mfiles/query_navigation.jpg" style="width:100%"></img>
          </picture>
          <p>
            On our robot, we load the CLIP-Field to help with the localization and navigation of the robot.
            When the robot gets a new text query, we first convert it to a representation vector by encoding it with
            sentence-BERT and CLIP-text encoder.
            Then, we compare the representations with the representations of the XYZ coordinates in the scene and find
            the location in space maximizing their similarity.
            We use the robot’s navigation stack to navigate to that region, and point the robot camera to an XYZ
            coordinate where the dot product was highest.
            We consider the navigation task successful if the robot can navigate to and point the camera at an object
            that satisfies the query.
          </p>
        </div>
      </div>
    </div>

    <!--Future Work-->
    <div class="container" style="padding-top: 20px">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Future Work</h2>
          <p>We showed that CLIP-Fields can learn 3D semantic scene representations from little or no labeled data,
            relying on weakly-supervised web-image trained models, and that we can use these model in order to perform a
            simple “look-at” task. CLIP-Fields allow us to answer queries of varying levels of complexity. We expect
            this kind of 3D representation to be generally useful for robotics. For example, it ma be enriched with
            affordances for planning; the geometric database can be readily combined with end-to-end differentiable
            planners. In future work, we also hope to explore models that share parameters across scenes, and can handle
            dynamic scenes and objects.
          </p>

        </div>
      </div>
    </div>

    <div class="container" style="padding-bottom: 150px; padding-top: 20px">
      <div class="row">
        <div class="col-12">
          <h6 id="footnote">
            † This work was done while the first author was interning at Facebook AI Research.
          </h6>
        </div>
      </div>
    </div>

  </div>
  <footer>
  </footer>
</body>
<script>
  var abstract_tag = document.querySelector("#abstract_tag");
  var abstract_text = document.querySelector("#abstract_text");
  abstract_text.style.display = "none";
  abstract_tag.style.cursor = "pointer";
  abstract_tag.style.color = "#dd00ff";
  var hide_logo = document.querySelector("#hide_logo");
  abstract_tag.addEventListener("click", function () {
    abstract_text.style.display = abstract_text.style.display == "none" ? "block" : "none";
    hide_logo.innerHTML = hide_logo.innerHTML == "↓" ? "↑" : "↓";
  });

  document.getElementById("replay_1").style.opacity = 0;
  document.getElementById("replay_1").style.cursor = "pointer";
  progressLoop("nyu_robot_run", "progress_nyu");
  progressLoop("pit_robot_run", "progress_pit");

</script>

</html>