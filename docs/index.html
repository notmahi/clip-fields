<!DOCTYPE html>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
</script>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta property="og:title" content="CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory">
  <meta property="og:description" content="Behavior Transformers: Cloning k modes with one stone">
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory">
  <meta property="og:image" content="https://notmahi.github.io/bet/mfiles/arch bet.001.png" />
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory">
  <meta name="twitter:description"
    content="Behavior Transformer (BeT), a new technique to model unlabeled demonstration data with multiple modes.">
  <meta name="twitter:image" content="https://notmahi.github.io/bet/mfiles/arch bet.001.png" />
  <meta name="twitter:creator" content="@notmahi" />
  <link rel="shortcut icon" href="img/favicon.png">
  <link rel="stylesheet" href="css/simple-grid.css">
  <title>CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory</title>
     <script>
      function retime(duration, class_name) {
        var videos = document.getElementsByClassName(class_name);
        for (var i = 0; i < videos.length; i++) {
          var video = videos[i];
          video.onloadeddata = function () {
            this.playbackRate = this.duration / duration;
          };
        }
      }

      function monitor(replay_name) {
        var div = document.getElementById(replay_name);
        div.style.opacity = 1.0;
      }

      function replay(class_name, replay_name) {
        var video = document.getElementById(class_name);
        video.currentTime = 0;
        video.play();
        var div = document.getElementById(replay_name);
        div.style.opacity = 0.0;
      }

      function seek(class_id, progress_id, time, endtime) {
        var video = document.getElementById(class_id);
        var progress = document.getElementById(progress_id);
        video.currentTime = time;
        video.supposedEndTime = endtime;
        video.supposedStartTime = time;
        progress.style.transitionDuration = "0s";
        progress.value = Math.round((video.currentTime / video.duration) * 100);
        progress.style.transitionDuration = "0.5s";
        video.addEventListener("timeupdate", function (event) {
          target = event.target;
          if (target.currentTime >= target.supposedEndTime) {
            target.currentTime = target.supposedStartTime; // change time index here
          }
        }, false);
      }

      function progressLoop(video_class_id, progress_id) {
        var video = document.getElementById(video_class_id);
        var progress = document.getElementById(progress_id);
        console.log(video.currentTime / video.duration);
        function innerLoop() {
          if (video !== null && (video.currentTime / video.duration)) {
            progress.value = Math.round((video.currentTime / video.duration) * 100);
          }
          window.requestAnimationFrame(innerLoop);
        }
        innerLoop();
      }
    </script>
    <style>
      .replay {
        font-size: 1.5em;
        color: #00A2FF;
        text-decoration: none;
      }
    </style>
</head>

<body>
  <div class="jumbotron">
    <div class="container">
      <div class="row">
        <div class="col-12 center">
          <h1>CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory</h1>
        </div>
        <div class="col-2 hidden-sm"></div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="https://arxiv.org/abs/2206.11251">
            <h3 style="color: #dd00ff">Paper</h3>
          </a>
        </div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="https://github.com/notmahi/bet">
            <h3 style="color: #dd00ff">Code</h3>
          </a>
        </div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="https://osf.io/983qz/">
            <h3 style="color: #dd00ff">Data</h3>
          </a>
        </div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="more/bibtex.txt">
            <h3 style="color: #dd00ff">Bibtex</h3>
          </a>
        </div>
      </div>
      <div class="row">
        <div class="col-4 center">
          <p><a href="https://mahis.life">Mahi Shafiullah<sup>1</sup></a></p>
        </div>
        <div class="col-4 center">
          <p><a href="https://cpaxton.github.io/">Chris Paxton<sup>2</sup></a></p>
        </div>
        <div class="col-4 center">
          <p><a href="https://lerrelpinto.com">Lerrel Pinto<sup>1</sup></a></p>
        </div>
        <div class="col-2 center">
          <p></p>
        </div>
        <div class="col-4 center">
          <p><a href="https://soumith.ch/">Soumith Chintala<sup>2</sup></a></p>
        </div>
        <div class="col-4 center">
          <p><a href="#">Arthur Szlam<sup>2</sup></a></p>
        </div>

      </div>
      <div class="row">
        <div class="col-6 center">
          <h3>1: New York University</h3>
        </div>
        <div class="col-6 center">
          <h3>2: Meta AI</h3>
        </div>
      </div>

      <!--Abstract-->
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom" id="abstract_tag">Abstract <span id="hide_logo">↓</span></h2>
          <p id="abstract_text">
            We propose CLIP-Fields, an implicit scene model that can be trained with no direct human supervision.
            This model learns a mapping from spatial locations to semantic embedding vectors.
            The mapping can then be used for a variety of tasks, such as segmentation, instance identification, semantic
            search over space, and view localization.
            Most importantly, the mapping can be trained with supervision coming only from web-image and web-text
            trained models such as CLIP, Detic, and Sentence-BERT.
            When compared to baselines like Mask-RCNN, our method outperforms on few-shot instance identification or
            semantic segmentation on the HM3D dataset with only a fraction of the examples.
            Finally, we show that using CLIP-Fields as a scene memory, robots can perform semantic navigation in
            real-world environments.
          </p>
        </div>
      </div>
    </div>
    <!--Videos-->
    <div class="container">
      <div class="row">
        <div class="col-12" style="width: 100%">
          <h2 class="center m-bottom" id="experiment_tag">Real World Robot Experiments</h2>
          <div class="col-12 img" id="nyu_hero_video">
            <h4>Robot queries in a real lab kitchen setup.</h4>
            <video id="nyu_robot_run" class="center" style="width: 100%" muted autoplay loop>
              <source src="./mfiles/nyu_robot_run_clipped_small.mp4" type="video/mp4">
            </video>
            <progress id="progress_nyu" max="100" value="0">Progress</progress>
            <div style="margin: none; width: 100%;">
              <button onclick="seek('nyu_robot_run', 'progress_nyu',  0, 17.33)" class="seek_button" id="nyu_wash"
                style="width: calc((1733% - 0%) / 55.75);">Wash my dishes</button>
              <button onclick="seek('nyu_robot_run', 'progress_nyu',  17.34, 29.00)" class="seek_button" id="nyu_trash"
                style="width: calc((2900% - 1734%) / 55.75);">Throw out my
                trash</button>
              <button onclick=" seek('nyu_robot_run', 'progress_nyu', 29.05, 43.2)" class="seek_button" id="nyu_coffee"
                style="width: calc((4320% - 2905%) / 55.75);">Make me a
                coffee</button>
              <button onclick="seek('nyu_robot_run', 'progress_nyu',  43.21, 55.03)" class="seek_button" id="nyu_lunch"
                style="width: calc((5503% - 4321%) / 55.75);">Warm up my
                lunch</button>
              <button onclick="seek('nyu_robot_run', 'progress_nyu',  0, 55.03)" class="seek_button" id="nyu_full"
                style="width: calc((5550% - 0%) / 55.75);">Full video</button>
            </div>
          </div>
          <div class="col-12 img" id="pit_hero_video">
            <h4>Robot queries in a real lounge/library setup.</h4>
            <video id="pit_robot_run" class="center" style="width: 100%" muted autoplay loop>
              <source src="./mfiles/pit_robot_run_clipped_small.mp4" type="video/mp4">
            </video>

            <progress id="progress_pit" max="100" value="0">Progress</progress>
            <div style="margin: none; width: 100%;">
              <button onclick="seek('pit_robot_run', 'progress_pit',  0, 9.66)" class="seek_button" id="pit_bookshelf"
                style="width: calc(966% / 58.5);">Bookshelf</button>
              <button onclick="seek('pit_robot_run', 'progress_pit',  9.67, 31.5)" class="seek_button" id="pit_relax"
                style="width: calc((3150% - 967%) / 58.5);">Sit down and
                relax</button>
              <button onclick="seek('pit_robot_run', 'progress_pit',  31.52, 43.33)" class="seek_button" id="pit_write"
                style="width: calc((4333% - 3152%) / 58.5);">Write a novel</button>
              <button onclick="seek('pit_robot_run', 'progress_pit',  43.34, 57.9)" class="seek_button" id="pit_putdown"
                style="width: calc((5790% - 4335%) / 58.5);">Put down my
                novel</button>
              <button onclick="seek('pit_robot_run', 'progress_pit',  0, 57.9)" class="seek_button" id="pit_full"
                style="width: 100%;">Full
                video</button>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!--Image-->
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Method</h2>
          <p>CLIP-Fields is based off of a simple idea:
          <ul style="font-size: 1.125rem;font-weight: 200;line-height: 1.8">
            <li>Webscale models like CLIP and Detic provide lots of semantic information about objects that can be used for robot tasks, but don't encode spatial qualities of this information.
            </li>
            <li>NeRF-like approaches, on the other hand, have recently shown that they can capture very detailed scene information.
	    </li>
            <li>We can combine these two, using a novel contrastive loss in order to capture scene-specific embeddings. We supervise multiple "heads," including object detection and CLIP, based on these webscale vision models, which allows us to do open-vocabulary queries at test time.
	    </li>
          </ul>
          </p>
        </div>
      </div>
    </div>
    <div class="container">
      <div class="row">
        <div class="col-12 center img">
          <video style="width: 100%" id="desc_1" onended="monitor('replay_1')" playsinline muted autoplay>
            <source src="./mfiles/1.mp4" type="video/mp4">
          </video>
          <span class="replay" id="replay_1" onclick="replay('desc_1', 'replay_1')">replay</span>
          <p>
            We use a k-means based clustering to cluster continuous actions into discrete bins.
            The bin centers, learned from the offline data, are used to convert each continous actions into a discrete
            and a continuous component.
            These components can be recombined into a full, continous action at any time.
          </p>
        </div>
      </div>
    </div>

    <div class="container">
      <div class="row">
        <div class="col-12 center img">
          <img class="center" src="./mfiles/data_processing.png" style="width:100%"></img>
          <span class="replay" id="replay_2" onclick="replay('desc_2', 'replay_2')">replay</span>
          <p>
            Our MinGPT model learns to predict a categorical distribution over the bins, as well as a residual
            continous component of an actions given bins.
            We train the bin predictor part using a negative-log likelihood based <a
              href="https://paperswithcode.com/method/focal-loss">Focal loss</a>, and the residual action predictor part
            using a <a
              href="https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/#loss-function">multi-task
              loss</a>.
          </p>
        </div>
      </div>
    </div>

    <div class="container">
      <div class="row">
        <div class="col-12 center img">
          <img class="center" src="./mfiles/arch.png" style="width:100%"></img>
          <p>
            During test, our model predicts a bin, and then uses the bin center and the associated residual continous
            action to reconstruct a full continous action to execute in the environment.
          </p>
        </div>
      </div>
    </div>

    <!--Experiments-->
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Experiments</h2>
          <p>Performance of BeT compared with different baselines in learning from demonstrations. For CARLA, we
            measure
            the
            probability of the car reaching the goal successfully. For Block push, we measure the probability of
            reaching one and
            two blocks, and the probabilities of pushing one and two blocks to respective squares. For Kitchen, we
            measure the
            probability of <math display="inline">
              <mi>n</mi>
            </math> tasks being completed by the model within the allotted 280 times. Evaluations are over 100
            rollouts
            in CARLA and 1,000 rollouts in Block push and Kitchen environments.
          </p>
          <img class="center" src="./mfiles/Exp Table.001.png" style="width:100%"></img>
          <p>Distribution of most frequent tasks completed in sequence in the Kitchen environment. Each task is
            colored
            differently,
            and frequency is shown out of a 1,000 unconditional rollouts from the models.
          </p>
          <br><br>
          <img class="center" src="./mfiles/multimodal_colorbar_flipped-1.png" style="width:100%"></img>
        </div>
      </div>
    </div>

    <!--Future Work-->
    <div class="container" style="padding-bottom: 150px; padding-top: 20px">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Future Work</h2>
          <p>In this work, we introduce Behavior Transformers (BeT), which uses a transformer-decoder based
            backbone with a discrete action mode predictor coupled with a continuous action offset corrector
            to model continuous actions sequences from open-ended, multi-modal demonstrations. While
            BeT shows promise, the truly exciting use of it would be to learn diverse behavior from human
            demonstrations or interactions in the real world. In parallel, extracting a particular, unimodal
            behavior
            policy from BeT during online interactions, either by distilling the model or by generating the right
            "prompts", would make BeT tremendously useful as a prior for online Reinforcement Learning.
          </p>
        </div>
      </div>
    </div>

  </div>
  <footer>
  </footer>
</body>
<script>
  var abstract_tag = document.querySelector("#abstract_tag");
  var abstract_text = document.querySelector("#abstract_text");
  var hide_logo = document.querySelector("#hide_logo");
  abstract_tag.addEventListener("click", function () {
    abstract_text.style.display = abstract_text.style.display == "none" ? "block" : "none";
    hide_logo.innerHTML = hide_logo.innerHTML == "↓" ? "↑" : "↓";
  });
  progressLoop("nyu_robot_run", "progress_nyu");
  progressLoop("pit_robot_run", "progress_pit");

</script>

</html>
